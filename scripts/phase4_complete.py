#!/usr/bin/env python3
"""
Script completo para la Fase 4 - Concurrencia y Tolerancia a Fallos.
Ejecuta todas las pruebas de la Fase 4 y genera un reporte final.
"""
import asyncio
import json
import time
from pathlib import Path
from datetime import datetime

# Importar m√≥dulos de prueba de la Fase 4
from concurrency_test import ConcurrencyTester
from fault_tolerance_test import FaultToleranceTester
from performance_metrics import PerformanceMonitor

class Phase4CompleteTest:
    """Coordinador completo de pruebas de la Fase 4."""
    
    def __init__(self):
        self.test_results = {
            "phase4_summary": {
                "start_time": None,
                "end_time": None,
                "total_duration": 0,
                "tests_executed": [],
                "overall_success": False
            },
            "concurrency_results": {},
            "fault_tolerance_results": {},
            "performance_results": {},
            "final_assessment": {}
        }
    
    async def run_complete_phase4_test(self):
        """Ejecuta la suite completa de pruebas de la Fase 4."""
        print("üöÄ INICIANDO FASE 4 COMPLETA - CONCURRENCIA Y TOLERANCIA A FALLOS")
        print("="*80)
        
        start_time = time.time()
        self.test_results["phase4_summary"]["start_time"] = datetime.now().isoformat()
        
        try:
            # Verificar estado inicial de la red
            if not await self._verify_initial_network_state():
                print("‚ùå Red P2P no est√° en estado adecuado para pruebas")
                return False
            
            # Fase 4.1: Pruebas de Concurrencia
            print("\n" + "üîÑ FASE 4.1 - PRUEBAS DE CONCURRENCIA" + "="*50)
            concurrency_success = await self._run_concurrency_tests()
            self.test_results["phase4_summary"]["tests_executed"].append("concurrency")
            
            # Pausa entre fases
            await asyncio.sleep(5)
            
            # Fase 4.2: Tolerancia a Fallos
            print("\n" + "üõ°Ô∏è  FASE 4.2 - TOLERANCIA A FALLOS" + "="*50)
            fault_tolerance_success = await self._run_fault_tolerance_tests()
            self.test_results["phase4_summary"]["tests_executed"].append("fault_tolerance")
            
            # Pausa entre fases
            await asyncio.sleep(5)
            
            # Fase 4.3: An√°lisis de Rendimiento
            print("\n" + "üìä FASE 4.3 - AN√ÅLISIS DE RENDIMIENTO" + "="*50)
            performance_success = await self._run_performance_analysis()
            self.test_results["phase4_summary"]["tests_executed"].append("performance")
            
            # Fase 4.4: Evaluaci√≥n Final
            print("\n" + "üìà FASE 4.4 - EVALUACI√ìN FINAL" + "="*50)
            final_assessment = await self._conduct_final_assessment()
            
            # Determinar √©xito general
            overall_success = all([
                concurrency_success,
                fault_tolerance_success, 
                performance_success,
                final_assessment["system_ready_for_production"]
            ])
            
            self.test_results["phase4_summary"]["overall_success"] = overall_success
            
        except Exception as e:
            print(f"‚ùå Error durante ejecuci√≥n de Fase 4: {e}")
            overall_success = False
        
        finally:
            # Finalizar y generar reporte
            end_time = time.time()
            self.test_results["phase4_summary"]["end_time"] = datetime.now().isoformat()
            self.test_results["phase4_summary"]["total_duration"] = end_time - start_time
            
            await self._generate_final_report()
            
            return overall_success
    
    async def _verify_initial_network_state(self) -> bool:
        """Verifica que la red P2P est√© en estado adecuado."""
        print("üîç Verificando estado inicial de la red P2P...")
        
        nodes = {
            "peer1": "127.0.0.1:50001",
            "peer2": "127.0.0.1:50002",
            "peer3": "127.0.0.1:50003"
        }
        
        # Usar el monitor de rendimiento para verificar salud
        monitor = PerformanceMonitor()
        snapshot = await monitor._take_metrics_snapshot()
        
        if snapshot.active_nodes < 3:
            print(f"‚ö†Ô∏è  Solo {snapshot.active_nodes}/3 nodos activos")
            print("Ejecuta: python scripts/start_nodes.py")
            return False
        
        print(f"‚úÖ Red verificada: {snapshot.active_nodes}/3 nodos activos")
        
        # Verificar funcionalidad b√°sica
        from scripts.test_client import P2PTestClient
        client = P2PTestClient()
        
        # Probar b√∫squeda b√°sica
        search_result = await client.search_file(nodes["peer1"], "video_especial.mp4")
        if not search_result.get("success"):
            print("‚ùå Funcionalidad de b√∫squeda no operativa")
            return False
        
        print("‚úÖ Funcionalidad b√°sica verificada")
        return True
    
    async def _run_concurrency_tests(self) -> bool:
        """Ejecuta pruebas de concurrencia."""
        try:
            tester = ConcurrencyTester()
            
            # Ejecutar pruebas de concurrencia con diferentes niveles
            concurrency_levels = [5, 10, 20]
            
            for level in concurrency_levels:
                print(f"\nüîÑ Prueba de concurrencia - Nivel {level}")
                
                # B√∫squedas concurrentes
                search_results = await tester.concurrent_searches(level)
                
                # Indexaciones concurrentes  
                index_results = await tester.concurrent_indexing(level)
                
                # Descargas concurrentes (si gRPC est√° disponible)
                try:
                    download_results = await tester.concurrent_downloads(min(level, 5))
                except Exception as e:
                    print(f"‚ö†Ô∏è  Descargas gRPC no disponibles: {e}")
                    download_results = {}
                
                await asyncio.sleep(2)
            
            # Guardar resultados
            tester.save_results("phase4_concurrency_results.json")
            self.test_results["concurrency_results"] = tester.results
            
            # Evaluar √©xito (tasa de √©xito > 80% en todas las pruebas)
            success_rates = []
            for result_type in ["searches", "indexing", "downloads"]:
                if tester.results[result_type]:
                    rates = [r["success_rate"] for r in tester.results[result_type]]
                    success_rates.extend(rates)
            
            avg_success_rate = sum(success_rates) / len(success_rates) if success_rates else 0
            concurrency_success = avg_success_rate >= 80.0
            
            print(f"üìä Tasa de √©xito promedio en concurrencia: {avg_success_rate:.1f}%")
            print(f"{'‚úÖ' if concurrency_success else '‚ùå'} Pruebas de concurrencia: {'EXITOSAS' if concurrency_success else 'FALLIDAS'}")
            
            return concurrency_success
            
        except Exception as e:
            print(f"‚ùå Error en pruebas de concurrencia: {e}")
            return False
    
    async def _run_fault_tolerance_tests(self) -> bool:
        """Ejecuta pruebas de tolerancia a fallos."""
        try:
            tester = FaultToleranceTester()
            
            # Pruebas de recuperaci√≥n de nodos
            node_failure_results = await tester.test_node_failure_recovery()
            
            # Pruebas de partici√≥n de red
            partition_results = await tester.test_network_partition_tolerance()
            
            # Pruebas de estr√©s gradual
            stress_results = await tester.test_gradual_node_stress()
            
            # Guardar resultados
            tester.save_results("phase4_fault_tolerance_results.json")
            self.test_results["fault_tolerance_results"] = tester.fault_results
            
            # Evaluar √©xito
            node_failures_survived = all(r["network_survived"] for r in node_failure_results)
            full_recovery = all(r["full_recovery"] for r in node_failure_results)
            partition_recovery = partition_results.get("recovery_successful", False)
            stress_resilience = all(r["network_healthy"] for r in stress_results)
            
            fault_tolerance_success = all([
                node_failures_survived,
                full_recovery,
                partition_recovery,
                stress_resilience
            ])
            
            print(f"üõ°Ô∏è  Red sobrevivi√≥ fallos de nodos: {'‚úÖ' if node_failures_survived else '‚ùå'}")
            print(f"üîÑ Recuperaci√≥n completa: {'‚úÖ' if full_recovery else '‚ùå'}")
            print(f"üåê Tolerancia a particiones: {'‚úÖ' if partition_recovery else '‚ùå'}")
            print(f"üí™ Resistencia a estr√©s: {'‚úÖ' if stress_resilience else '‚ùå'}")
            print(f"{'‚úÖ' if fault_tolerance_success else '‚ùå'} Tolerancia a fallos: {'EXITOSA' if fault_tolerance_success else 'FALLIDA'}")
            
            return fault_tolerance_success
            
        except Exception as e:
            print(f"‚ùå Error en pruebas de tolerancia a fallos: {e}")
            return False
    
    async def _run_performance_analysis(self) -> bool:
        """Ejecuta an√°lisis de rendimiento."""
        try:
            monitor = PerformanceMonitor(monitoring_duration=180)  # 3 minutos
            
            # Monitoreo continuo
            await monitor.start_continuous_monitoring()
            
            # Benchmark de operaciones
            benchmark_results = await monitor.benchmark_operations()
            
            # Medici√≥n de latencia
            latency_matrix = await monitor.measure_network_latency()
            
            self.test_results["performance_results"] = {
                "benchmark_results": benchmark_results,
                "latency_matrix": latency_matrix,
                "monitoring_completed": True
            }
            
            # Evaluar rendimiento
            search_performance = benchmark_results.get("search_operations", {})
            search_ops_per_sec = search_performance.get("ops_per_second", 0)
            avg_response_time = search_performance.get("avg_time", float('inf'))
            
            # Criterios de √©xito: > 10 b√∫squedas/seg y < 1s respuesta promedio
            performance_success = search_ops_per_sec >= 10.0 and avg_response_time <= 1.0
            
            print(f"üöÄ B√∫squedas por segundo: {search_ops_per_sec:.1f}")
            print(f"‚ö° Tiempo de respuesta promedio: {avg_response_time:.3f}s")
            print(f"{'‚úÖ' if performance_success else '‚ùå'} An√°lisis de rendimiento: {'EXITOSO' if performance_success else 'INSUFICIENTE'}")
            
            return performance_success
            
        except Exception as e:
            print(f"‚ùå Error en an√°lisis de rendimiento: {e}")
            return False
    
    async def _conduct_final_assessment(self) -> Dict[str, Any]:
        """Conduce evaluaci√≥n final del sistema."""
        print("üéØ Realizando evaluaci√≥n final del sistema P2P...")
        
        # Criterios de evaluaci√≥n
        assessment_criteria = {
            "concurrency_support": False,
            "fault_tolerance": False,
            "performance_acceptable": False,
            "scalability_ready": False,
            "production_ready": False
        }
        
        # Evaluar concurrencia
        concurrency_results = self.test_results.get("concurrency_results", {})
        if concurrency_results.get("searches"):
            avg_concurrency_success = sum(r["success_rate"] for r in concurrency_results["searches"]) / len(concurrency_results["searches"])
            assessment_criteria["concurrency_support"] = avg_concurrency_success >= 85.0
        
        # Evaluar tolerancia a fallos
        fault_results = self.test_results.get("fault_tolerance_results", {})
        node_failures = fault_results.get("node_failures", [])
        if node_failures:
            fault_success = all(r["network_survived"] and r["full_recovery"] for r in node_failures)
            assessment_criteria["fault_tolerance"] = fault_success
        
        # Evaluar rendimiento
        performance_results = self.test_results.get("performance_results", {})
        benchmark = performance_results.get("benchmark_results", {})
        if benchmark.get("search_operations"):
            ops_per_sec = benchmark["search_operations"].get("ops_per_second", 0)
            assessment_criteria["performance_acceptable"] = ops_per_sec >= 10.0
        
        # Evaluar escalabilidad (basado en rendimiento bajo estr√©s)
        if fault_results.get("resilience_metrics"):
            stress_tests = fault_results["resilience_metrics"]
            high_stress_success = any(r["success_rate"] >= 70.0 and r["stress_level"] >= 20 for r in stress_tests)
            assessment_criteria["scalability_ready"] = high_stress_success
        
        # Evaluaci√≥n general para producci√≥n
        assessment_criteria["production_ready"] = all([
            assessment_criteria["concurrency_support"],
            assessment_criteria["fault_tolerance"],
            assessment_criteria["performance_acceptable"]
        ])
        
        # Mostrar evaluaci√≥n
        print("\nüìã EVALUACI√ìN FINAL:")
        print(f"üîÑ Soporte de concurrencia: {'‚úÖ' if assessment_criteria['concurrency_support'] else '‚ùå'}")
        print(f"üõ°Ô∏è  Tolerancia a fallos: {'‚úÖ' if assessment_criteria['fault_tolerance'] else '‚ùå'}")
        print(f"‚ö° Rendimiento aceptable: {'‚úÖ' if assessment_criteria['performance_acceptable'] else '‚ùå'}")
        print(f"üìà Listo para escalabilidad: {'‚úÖ' if assessment_criteria['scalability_ready'] else '‚ùå'}")
        print(f"üöÄ Listo para producci√≥n: {'‚úÖ' if assessment_criteria['production_ready'] else '‚ùå'}")
        
        final_assessment = {
            "criteria": assessment_criteria,
            "system_ready_for_production": assessment_criteria["production_ready"],
            "recommendations": self._generate_recommendations(assessment_criteria)
        }
        
        self.test_results["final_assessment"] = final_assessment
        return final_assessment
    
    def _generate_recommendations(self, criteria: Dict[str, bool]) -> List[str]:
        """Genera recomendaciones basadas en los criterios de evaluaci√≥n."""
        recommendations = []
        
        if not criteria["concurrency_support"]:
            recommendations.append("Mejorar manejo de concurrencia - considerar pool de conexiones")
        
        if not criteria["fault_tolerance"]:
            recommendations.append("Implementar mecanismos adicionales de recuperaci√≥n autom√°tica")
        
        if not criteria["performance_acceptable"]:
            recommendations.append("Optimizar algoritmos de b√∫squeda y comunicaci√≥n entre nodos")
        
        if not criteria["scalability_ready"]:
            recommendations.append("Implementar balanceador de carga y manejo de nodos din√°mico")
        
        if criteria["production_ready"]:
            recommendations.append("Sistema listo para deployment - considerar monitoreo continuo")
        else:
            recommendations.append("Resolver issues cr√≠ticos antes de deployment en producci√≥n")
        
        return recommendations
    
    async def _generate_final_report(self):
        """Genera reporte final completo de la Fase 4."""
        print("\nüìä GENERANDO REPORTE FINAL DE LA FASE 4...")
        
        # Guardar resultados completos
        with open("phase4_complete_results.json", "w") as f:
            json.dump(self.test_results, f, indent=2)
        
        # Generar reporte en texto plano
        report_path = "PHASE4_FINAL_REPORT.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("# REPORTE FINAL - FASE 4: CONCURRENCIA Y TOLERANCIA A FALLOS\n\n")
            
            f.write("## üìã Resumen Ejecutivo\n\n")
            summary = self.test_results["phase4_summary"]
            f.write(f"- **Duraci√≥n total**: {summary['total_duration']:.1f} segundos\n")
            f.write(f"- **Pruebas ejecutadas**: {', '.join(summary['tests_executed'])}\n")
            f.write(f"- **√âxito general**: {'‚úÖ S√ç' if summary['overall_success'] else '‚ùå NO'}\n\n")
            
            f.write("## üîÑ Resultados de Concurrencia\n\n")
            concurrency = self.test_results.get("concurrency_results", {})
            if concurrency.get("searches"):
                avg_search_success = sum(r["success_rate"] for r in concurrency["searches"]) / len(concurrency["searches"])
                f.write(f"- **Tasa de √©xito promedio en b√∫squedas**: {avg_search_success:.1f}%\n")
            
            f.write("\n## üõ°Ô∏è Resultados de Tolerancia a Fallos\n\n")
            fault_tolerance = self.test_results.get("fault_tolerance_results", {})
            node_failures = fault_tolerance.get("node_failures", [])
            if node_failures:
                survived_all = all(r["network_survived"] for r in node_failures)
                recovered_all = all(r["full_recovery"] for r in node_failures)
                f.write(f"- **Red sobrevivi√≥ a todos los fallos**: {'‚úÖ S√ç' if survived_all else '‚ùå NO'}\n")
                f.write(f"- **Recuperaci√≥n completa**: {'‚úÖ S√ç' if recovered_all else '‚ùå NO'}\n")
            
            f.write("\n## üìä Resultados de Rendimiento\n\n")
            performance = self.test_results.get("performance_results", {})
            benchmark = performance.get("benchmark_results", {})
            if benchmark.get("search_operations"):
                ops_per_sec = benchmark["search_operations"].get("ops_per_second", 0)
                avg_time = benchmark["search_operations"].get("avg_time", 0)
                f.write(f"- **B√∫squedas por segundo**: {ops_per_sec:.1f}\n")
                f.write(f"- **Tiempo promedio de respuesta**: {avg_time:.3f}s\n")
            
            f.write("\n## üéØ Evaluaci√≥n Final\n\n")
            assessment = self.test_results.get("final_assessment", {})
            if assessment.get("criteria"):
                for criterion, passed in assessment["criteria"].items():
                    f.write(f"- **{criterion.replace('_', ' ').title()}**: {'‚úÖ CUMPLE' if passed else '‚ùå NO CUMPLE'}\n")
            
            f.write("\n## üí° Recomendaciones\n\n")
            recommendations = assessment.get("recommendations", [])
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n")
        
        print(f"üìä Reporte final guardado en: {report_path}")
        print(f"üìä Datos completos en: phase4_complete_results.json")

async def main():
    """Funci√≥n principal para ejecutar la Fase 4 completa."""
    phase4_tester = Phase4CompleteTest()
    
    success = await phase4_tester.run_complete_phase4_test()
    
    print("\n" + "="*80)
    if success:
        print("üéâ FASE 4 COMPLETADA EXITOSAMENTE")
        print("‚úÖ El sistema P2P est√° listo para concurrencia y tolerancia a fallos")
    else:
        print("‚ùå FASE 4 COMPLETADA CON ISSUES")
        print("‚ö†Ô∏è  Revisar reporte para identificar √°reas de mejora")
    print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
